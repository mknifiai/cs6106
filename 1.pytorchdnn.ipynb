{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e078b07f-4b61-4ee4-ac72-48a1da4bc411",
   "metadata": {},
   "source": [
    "## Deep Neural Networks with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37054d96-ce8c-459b-8da8-462ea2dabe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4084311-ed7c-43cc-b15f-0c7b194183cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(2, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0325c4d6-31ec-4de7-91c4-b0dd1603e49c",
   "metadata": {},
   "source": [
    "### 1. Layers and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84e32d98-1a12-44b0-a78b-7c4b11abf722",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class nn.Module to perform\n",
    "        # the necessary initialization\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.out = nn.LazyLinear(10)\n",
    "        \n",
    "    # Define the forward propagation of the model, that is, how to return the\n",
    "    # required model output based on the input X\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611ba9d-3b95-4ded-b4cc-320ce74b11ea",
   "metadata": {},
   "source": [
    "#### How to integrate arbitrary code into the flow of your neural network computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "214fc58a-5dca-4cc0-bac1-8bbeb51e1612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd84617-91e7-41db-83bb-39236f8bb752",
   "metadata": {},
   "source": [
    "#### Nest modules in some creative ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1230a7a-a618-4e6d-9ba7-b6010a0f735e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2701, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
    "                                 nn.LazyLinear(32), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(16)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8523f2-ee68-4369-84ea-02586f4d8c41",
   "metadata": {},
   "source": [
    "### 2. Parameter Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76573ca3-171e-41df-a108-c32929688042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888b975-abae-425f-af98-fae1d5e26724",
   "metadata": {},
   "source": [
    "#### Parameter Access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a854ca-82a8-44c5-8984-a91d14f5b429",
   "metadata": {},
   "source": [
    "We can inspect the parameters of the second fully connected layer as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa941abc-a6bd-4acd-b701-3228785b3fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.3216, -0.0429,  0.2595, -0.2796,  0.0971, -0.0296, -0.3129, -0.1053]])),\n",
       "             ('bias', tensor([-0.2723]))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bf36db9-f8ee-423d-aeaf-f268fda47682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2723])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bd8e88e-7b1f-4f2a-88be-4b79cec8685e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight', torch.Size([8, 4])),\n",
       " ('0.bias', torch.Size([8])),\n",
       " ('2.weight', torch.Size([1, 8])),\n",
       " ('2.bias', torch.Size([1]))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, param.shape) for name, param in net.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069563f-2047-4034-a737-9ccddca15723",
   "metadata": {},
   "source": [
    "#### Tied Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dba5494-c054-4ba1-afd7-84905348a5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "shared = nn.LazyLinear(8)\n",
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "net(X)\n",
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# Make sure that they are actually the same object rather than just having the\n",
    "# same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310238fb-56d3-47c4-a02d-cfc509078baa",
   "metadata": {},
   "source": [
    "when parameters are tied what happens to the gradients? Since the model parameters contain gradients, the gradients of the second hidden layer and the third hidden layer are added together during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680520ac-111d-42d6-84ba-a330a4361d72",
   "metadata": {},
   "source": [
    "### 3. Parameter Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e857a0c-fb31-408d-bea7-3e580600e94d",
   "metadata": {},
   "source": [
    "- By default, PyTorch initializes weight and bias matrices **uniformly** by drawing from a range that is computed according to the input and output dimension. \n",
    "\n",
    "- **PyTorch’s nn.init module** provides a variety of preset initialization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "341da3ec-2fe4-4b01-8e45-8bde18ff101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b97fb93a-b2e6-425d-a064-c00ebd156fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3c58b-77fd-4c9e-9665-8bef08f8a0a4",
   "metadata": {},
   "source": [
    "- The code below initializes all weight parameters as Gaussian random variables with standard deviation 0.01, while bias parameters are cleared to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9082d287-e4fd-448b-979f-de07d9f5a092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0049, -0.0119, -0.0006,  0.0060]), tensor(0.))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.normal_(module.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(module.bias)\n",
    "        \n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff87698d-d0c4-4156-b2b8-ab3af361d114",
   "metadata": {},
   "source": [
    "- We can also initialize all the parameters to a given constant value (say, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5719deff-3748-4f50-b962-6a5dad164c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.constant_(module.weight, 1)\n",
    "        nn.init.zeros_(module.bias)\n",
    "        \n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7dcb6-9f13-4eae-9fbb-09d1902946a4",
   "metadata": {},
   "source": [
    "#### Built-in Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd77e6a-ac67-4040-a9f7-7d91c1d3c165",
   "metadata": {},
   "source": [
    "#### Xavier Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd452a8-003d-4379-941d-799d185ec0b1",
   "metadata": {},
   "source": [
    "- The Xavier initialization samples weights from a Gaussian distribution with **zero mean** and **variance** $\\sigma^{2} = \\frac{2}{n_{in}+n_{out}}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423208e9-4085-445b-ad15-b9af93162254",
   "metadata": {},
   "source": [
    "- Below we initialize the first layer with the Xavier initializer and initialize the second layer to a constant value of 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a955c0a-7da4-4a31-ac61-861d3c50b478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2814, -0.1015, -0.5769,  0.4441])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def init_xavier(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        \n",
    "def init_42(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.constant_(module.weight, 42)\n",
    "        \n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac06b5-8af5-43ad-979f-53800608c06a",
   "metadata": {},
   "source": [
    "### 4. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91249234-333a-40fe-b924-839ff7456a21",
   "metadata": {},
   "source": [
    "#### Sigmoid Function\n",
    "\n",
    "- The sigmoid function transforms those inputs whose values lie in the domain $R$, to outputs that lie on the interval (0, 1)\n",
    "  \n",
    "$sigmoid(x) = \\frac{1}{1 + exp(-x)} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a6315-0546-4b9f-bd61-69ee649b9b1a",
   "metadata": {},
   "source": [
    "#### Hyperbolic tangent (Tanh) Function\n",
    "\n",
    "- the tanh (hyperbolic tangent) function squashes its inputs, transforming them into elements on the interval between −1 and 1\n",
    "  \n",
    "$tanh(x) = \\frac{1 − exp(−2x)} {1 + exp(−2x)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2429ae20-88ec-4681-b953-e0eea452a3a7",
   "metadata": {},
   "source": [
    "#### Rectified linear unit (ReLU) Function\n",
    "\n",
    "- the ReLU function retains only positive elements and discards all negative elements by setting the corresponding activations to 0\n",
    "\n",
    "  \n",
    "$ReLU(x) = max(x, 0).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c41ce-a2d9-4992-bbd2-c6e98833452d",
   "metadata": {},
   "source": [
    "#### Leaky ReLU function\n",
    "\n",
    "\n",
    "<img src=\"img/lrelu.png\" width=200 height=200 />\n",
    "\n",
    "- For $\\alpha \\in (0, 1)$, leaky ReLU is a nonlinear function that give a non-zero output for a negative input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c79a7-b00a-4449-976a-be3fee191873",
   "metadata": {},
   "source": [
    "#### GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "GELU(x) = $x * Φ(x)$\n",
    "\n",
    "where $Φ(x)$ is the Cumulative Distribution Function for Gaussian Distribution.\n",
    "\n",
    "$GELU(x)=0.5* x * (1+Tanh(\\sqrt{\\frac{2}{π}}*(x+0.044715 * x^{3})))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26510b50-7512-4e5d-8eca-9a13076e0df8",
   "metadata": {},
   "source": [
    "#### Swish function\n",
    "\n",
    "- Applies the Sigmoid Linear Unit (SiLU) function, element-wise.\n",
    "\n",
    "The SiLU function is also known as the swish function.\n",
    "\n",
    "silu(x) $= x ∗ \\sigma(x)$\n",
    "\n",
    "\n",
    "where $\\sigma(x)$ is the logistic sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427db90-d00d-4632-972e-d86660d8902e",
   "metadata": {},
   "source": [
    "### 5. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d95d49-f738-483c-bccf-849df5256321",
   "metadata": {},
   "source": [
    "#### Adam\n",
    "\n",
    "- It uses exponential weighted moving averages (also known as leaky averaging) to obtain an estimate of both the momentum and also the second moment of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347354c0-2362-4970-83a1-4f1ccba64554",
   "metadata": {},
   "source": [
    "$v_{t} ← \\beta_{1} v_{t-1}+ (1 − \\beta_{1}) g_{t}, $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de617109-faea-419c-ba67-c0e1c667d0b0",
   "metadata": {},
   "source": [
    "$ s_{t} ← \\beta_{2} s_{t-1} + ( 1 − \\beta_{2} ) g_{t}^{2} $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6241278a-05b9-4d4a-b7d7-4fb46036f6cd",
   "metadata": {},
   "source": [
    "Here $\\beta_{1}$ and $\\beta_{2}$ are nonnegative weighting parameters. Common choices for them are\n",
    "$\\beta_{1}$ = 0.9 and $\\beta_{2}$ = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb5289-7412-42d4-8da9-464caab0ed5b",
   "metadata": {},
   "source": [
    "Then we rescale the gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be372938-e5b6-4da7-a3d5-26505443725d",
   "metadata": {},
   "source": [
    "$g_{t}^{'} = \\frac{\\lambda \\hat{v_{t}}}{\\sqrt{\\hat{s_{t}}} + \\epsilon}$, where"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f5381-3892-4f5c-9fc3-afe4a00dca70",
   "metadata": {},
   "source": [
    "$ v^{'} = \\frac{v_{t}}{1 - \\beta_{1}^{t}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d1e0fa-c27e-4df0-acba-9ec5f1947c8d",
   "metadata": {},
   "source": [
    "$ s^{'} = \\frac{s_{t}}{1 - \\beta_{2}^{t}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37d836-bc05-4dbd-9dda-c3bc2c998130",
   "metadata": {},
   "source": [
    "$x_{t} ← x_{t-1} − g_{t}^{'}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7928b-111d-430e-97ba-68f73ca562cd",
   "metadata": {},
   "source": [
    "- For gradients with significant variance we may encounter issues with convergence. They can be amended by using larger minibatches or by switching to an improved estimate for $s_{t}$ . Yogi offers such an alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c2af4c-3846-4077-ae56-b95d6c79ace4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
