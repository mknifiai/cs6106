{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc5f9b3-56b2-411a-b6ca-0a96b19a54ec",
   "metadata": {},
   "source": [
    "## 1. Residual Networks (ResNet) and ResNeXt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d2f22-f7f1-4617-92a3-08f84a33f79d",
   "metadata": {},
   "source": [
    "when design deeper networks, what important is the ability to design networks where adding layers makes networks strictly **more expressive rather than just different.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8656f-8ef2-46c1-bb75-bc0db3415492",
   "metadata": {},
   "source": [
    "#### Function Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc82a6e3-7058-42c5-9ce9-c0c681e27395",
   "metadata": {},
   "source": [
    "given a dataset with features $X$ and labels $y$, we might try finding $ f_{F}^{*} $ by solving the following optimization problem:\n",
    "\n",
    "<img src=\"img/funclass.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6078077-ba39-4b2d-aebf-9fcc29346361",
   "metadata": {},
   "source": [
    "<img src=\"img/nest.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50967eba-8cdc-4bdd-bf6a-c275bd8c868f",
   "metadata": {},
   "source": [
    "- If larger function classes contain the smaller ones we are guaranteed that increasing them strictly increases the expressive power of the network.\n",
    "\n",
    "- **For deep neural networks**, if we can train the newly-added layer into an identity function $f(x) = x$, the new model will be as effective as the original model. As the new model may get a better solution to fit the training dataset, the added layer might make it easier to reduce training errors.\n",
    "\n",
    "  \n",
    "- **The idea behind residual network** (ResNet) is that every additional layer should more easily contain **the identity function** as one of its elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a082f-f643-4e12-9e82-41778f9bc9f9",
   "metadata": {},
   "source": [
    "### Residual Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3293bbbd-ac4d-4261-b676-74d4aafcc22c",
   "metadata": {},
   "source": [
    "<img src=\"img/res.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586c1a1-03a6-4135-90b8-5489b6f1cb84",
   "metadata": {},
   "source": [
    "- The right figure illustrates the **residual block of ResNet**, where the solid line carrying the layer input x to the addition operator is called a **residual connection** (or shortcut connection).\n",
    "\n",
    "- The portion within the dotted-line box needs to learn **the residual mapping** $g (x) = f (x)-x$ making **the identity mapping** $f (x) = x$ easier to learn.\n",
    "\n",
    "- the residual block can be thought of as a special case of the multi-branch Inception block: it has two branches one of which is the identity mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589a11ef-bd4a-4633-933e-1d04a2c5e0fb",
   "metadata": {},
   "source": [
    "#### ResNet block with and without 1 × 1 convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2113e9d2-b4f8-450b-8409-579416bad670",
   "metadata": {},
   "source": [
    "<img src=\"img/resnet.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c23444d-2bf0-4e8d-a8cc-b8eaaf556493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e3d82-2352-4d56-a4f2-90e3013ced64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module): \n",
    "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
    "                                   stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                        stride=strides)\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e946e-7789-4781-99bc-b41e8485f94d",
   "metadata": {},
   "source": [
    "#### ResNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1200a-e4c6-4c6f-b48c-37f1f64af4aa",
   "metadata": {},
   "source": [
    "<img src=\"img/resnetm.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa5dc05-cde9-45c3-9440-3f622d38264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40fd157-cd42-4e6b-aaa2-3e30734ea9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 1000]                 --\n",
       "├─Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         128\n",
       "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-5                        [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-6                        [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-3                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 28, 28]          73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-15                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 28, 28]          256\n",
       "│    │    └─Sequential: 3-18             [1, 128, 28, 28]          8,448\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-20                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-22                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-23                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 28, 28]          --\n",
       "├─Sequential: 1-7                        [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 14, 14]          294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 256, 14, 14]          512\n",
       "│    │    └─Sequential: 3-31             [1, 256, 14, 14]          33,280\n",
       "│    │    └─ReLU: 3-32                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-33                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-35                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-38                   [1, 256, 14, 14]          --\n",
       "├─Sequential: 1-8                        [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-7                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-39                 [1, 512, 7, 7]            1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-41                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-42                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 512, 7, 7]            1,024\n",
       "│    │    └─Sequential: 3-44             [1, 512, 7, 7]            132,096\n",
       "│    │    └─ReLU: 3-45                   [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-46                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-48                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-51                   [1, 512, 7, 7]            --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-10                           [1, 1000]                 513,000\n",
       "==========================================================================================\n",
       "Total params: 11,689,512\n",
       "Trainable params: 11,689,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.81\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 39.75\n",
       "Params size (MB): 46.76\n",
       "Estimated Total Size (MB): 87.11\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resNet_model = models.resnet18(pretrained=True)\n",
    "torchinfo.summary(resNet_model,(3, 224, 224),batch_dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab94975-ccd1-46d7-8af1-a8aab37636f2",
   "metadata": {},
   "source": [
    "### ResNeXt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9911390-5bb6-45e9-be92-8d40b501714f",
   "metadata": {},
   "source": [
    "- One of the challenges one encounters in the design of ResNet is the trade-off between non-linearity and dimensionality within a given block.\n",
    "    - meaning, we could add more nonlinearity by increasing the number of layers, or by increasing the width of the convolutions\n",
    "    - An alternative strategy is to increase the number of channels that can carry information between blocks. But this technique comes with a quadratic penalty.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Inspiration from the Inception block, **ResNeXt applies multiple independent groups to the ResNet block**\n",
    "- Different from the smorgasbord of transformations in Inception, **ResNeXt** adopts the same transformation in all branches, thus minimizing the need for manual tuning of each branch.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e6cd20-571c-4587-bf0d-4739ec16dd68",
   "metadata": {},
   "source": [
    "<img src=\"img/resnext.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d535f7a-ccca-441d-9f9d-bbfbcf203bf1",
   "metadata": {},
   "source": [
    "- Breaking up a convolution from $c_{i}$ to $c_{o}$ channels into one of $g$ groups of size $c_{i}/g$ generating $g$ outputs of size $c_{o}/g$ is called, **a grouped convolution**\n",
    "- The computational cost is reduced from $O(c_{i}. c_{o})$ to $O(g.(c_{i}/g).(c_{o}/g))$ $=$ $O(c_{i}. c_{o} /g )$, i.e., it is $g$ times faster.\n",
    "- the number of parameters needed to generate the output is also reduced from a $c_{i}$ x $c_{o}$ matrix to $g$ smaller matrices of size $(c_{i}/g)$ x $(c_{o}/g)$, again a $g$ times reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f425d7e-49a7-43a2-8ea1-c95bfddecf7c",
   "metadata": {},
   "source": [
    "- The only challenge in this design is that no information is exchanged between the $g$ groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c1de0-7d7a-467f-8c1d-827d3ca5958f",
   "metadata": {},
   "source": [
    "**The ResNeXt block** amends this in two ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9b46d8-9775-4d78-8200-2cbf2ff25767",
   "metadata": {},
   "source": [
    "- the grouped convolution with a 3 × 3 kernel is sandwiched in between two 1 × 1 convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75724a02-3dbd-49ad-94e9-00c56d1f97b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNeXtBlock(nn.Module):\n",
    "    \"\"\"The ResNeXt block.\"\"\"\n",
    "    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,\n",
    "                 strides=1):\n",
    "        super().__init__()\n",
    "        bot_channels = int(round(num_channels * bot_mul))\n",
    "        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)\n",
    "        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,\n",
    "                                   stride=strides, padding=1,\n",
    "                                   groups=bot_channels//groups)\n",
    "        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "        self.bn3 = nn.LazyBatchNorm2d()\n",
    "        if use_1x1conv:\n",
    "            self.conv4 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "            self.bn4 = nn.LazyBatchNorm2d()\n",
    "        else:\n",
    "            self.conv4 = None\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = F.relu(self.bn2(self.conv2(Y)))\n",
    "        Y = self.bn3(self.conv3(Y))\n",
    "        if self.conv4:\n",
    "            X = self.bn4(self.conv4(X))\n",
    "        return F.relu(Y + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791bb9d5-3126-4f2c-8ded-c2851a863b0c",
   "metadata": {},
   "source": [
    "## 2. DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb5059-6b01-4a7c-b0e3-0a72a990e336",
   "metadata": {},
   "source": [
    "- ResNet decomposes functions into:\n",
    "\n",
    "\n",
    ">> $f(x) = g(x) + x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91354ef-11fc-4ad6-8a5a-d948d3cc205b",
   "metadata": {},
   "source": [
    "ResNet decomposes $f$ into a simple linear term and a more complex nonlinear one. \n",
    "\n",
    "What if we wanted to capture (not necessarily add) information beyond two terms? One such solution is DenseNet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0ed20-5bd4-43ee-ac3a-837d8e804dee",
   "metadata": {},
   "source": [
    "<img src=\"img/res_vs_dens.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a93521f-0cd6-49d9-88ef-afcd8e90d069",
   "metadata": {},
   "source": [
    "- we perform a mapping from $x$ to its values after applying an increasingly complex sequence of functions:\n",
    "\n",
    "<img src=\"img/denseq.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4659445-c6ab-4531-be35-4825e8bb8e04",
   "metadata": {},
   "source": [
    "- The name DenseNet arises from the fact that the dependency graph between variables becomes quite dense. The final layer of such a chain is densely connected to all previous layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deaa527-58a6-4e41-87aa-f7195b4c7a4e",
   "metadata": {},
   "source": [
    "<img src=\"img/densgraph.png\" width=300 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0773b17d-dbe9-4eb2-810a-b4aaaf51f3e9",
   "metadata": {},
   "source": [
    ">>> Note how the dimensionality increases with depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98cc03b-54a6-4466-88a7-df1a7c322f7d",
   "metadata": {},
   "source": [
    "The main components that comprise a DenseNet are **dense blocks** and **transition layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf3712-d04f-49c1-9394-ff9406e95f43",
   "metadata": {},
   "source": [
    "**dense blocks**\n",
    "    \n",
    "    - define how the inputs and outputs are concatenated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89e3abbf-cab6-46f6-8eef-985e56b35161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "        nn.LazyConv2d(num_channels, kernel_size=3, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f38e6b87-00d6-45cd-8243-7061560be9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, num_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        layer = []\n",
    "        for i in range(num_convs):\n",
    "            layer.append(conv_block(num_channels))\n",
    "        self.net = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # Concatenate input and output of each block along the channels\n",
    "            X = torch.cat((X, Y), dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ee3947d-8558-4ff0-8c9b-430233e8c7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = DenseBlock(2, 10)\n",
    "X = torch.randn(4, 3, 8, 8)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ff9e8-1ee1-4e6f-bc17-40e927d0be8a",
   "metadata": {},
   "source": [
    "**transition layers**\n",
    "\n",
    "- control the number of channels so that it is not too large, since the expansion $ x \n",
    " \\rightarrow [x, f_{1}(x), f_{2} ([x, f_{1} (x)]),...]$ can be quite high-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b1f9c-099a-4049-bb70-2bb2c89ece8f",
   "metadata": {},
   "source": [
    "- it reduces the number of channels by using a 1 × 1 convolution.\n",
    "- Moreover, it halves the height and width via average pooling with a stride of 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "189e9638-5865-40a3-bb3b-c5266e71b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "        nn.LazyConv2d(num_channels, kernel_size=1),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e4e5d91-c4c9-4944-9f62-00c0b3f5ebbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = transition_block(10)\n",
    "blk(Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2fc67c-c935-4eb8-ab78-d45f390732e6",
   "metadata": {},
   "source": [
    "### DenseNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7cc8ab-1248-4eeb-ae02-201fc8bee19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.module):\n",
    "    def __init__(self, num_channels=64, growth_rate=32, arch=(4, 4, 4, 4),\n",
    "             lr=0.1, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(self.b1())\n",
    "        for i, num_convs in enumerate(arch):\n",
    "            \n",
    "            self.net.add_module(f'dense_blk{i+1}', DenseBlock(num_convs,\n",
    "                                                          growth_rate))\n",
    "            # The number of output channels in the previous dense block\n",
    "            num_channels += num_convs * growth_rate\n",
    "            \n",
    "            # A transition layer that halves the number of channels is added\n",
    "            # between the dense blocks\n",
    "            if i != len(arch) - 1:\n",
    "                num_channels //= 2\n",
    "                self.net.add_module(f'tran_blk{i+1}', transition_block(\n",
    "                    num_channels))\n",
    "        \n",
    "        self.net.add_module('last', nn.Sequential(\n",
    "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "            nn.LazyLinear(num_classes)))\n",
    "        self.net.apply(nn.init.xavier_uniform_)\n",
    "        \n",
    "        def b1(self):\n",
    "            return nn.Sequential(\n",
    "                nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "                nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9cbecd-fc24-4feb-ae69-419b0a266ab3",
   "metadata": {},
   "source": [
    "- here DenseNet uses four dense blocks.\n",
    "- we set the number of channels (i.e., **growth rate**) for the convolutional layers in the dense block to 32, so 128 channels will be added to each dense block.\n",
    "- we use the transition layer to halve the height and width and halve the number of channels.\n",
    "- a global pooling layer and a fully connected layer are connected at the end to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d26a57-cc66-46f8-968a-b256a1006284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
